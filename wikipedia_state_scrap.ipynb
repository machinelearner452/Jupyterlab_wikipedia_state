{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal is to obtaining the USA housing price driving factors, then map and graph that information. To have a starting point I will use the Wikipedia list of states and country in the USA. This is the code to screen scrap the current list from Wikipedia.\n",
    "\n",
    "#https://en.wikipedia.org/wiki/List_of_United_States_counties_and_county_equivalents\n",
    "#https://en.wikipedia.org/wiki/County_(United_States)\n",
    "\n",
    "Referance code used: https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   State\n",
      "0                Alabama\n",
      "1                 Alaska\n",
      "2                Arizona\n",
      "3               Arkansas\n",
      "4             California\n",
      "5               Colorado\n",
      "6            Connecticut\n",
      "7               Delaware\n",
      "8   District of Columbia\n",
      "9                Florida\n",
      "10               Georgia\n",
      "11                Hawaii\n",
      "12                 Idaho\n",
      "13              Illinois\n",
      "14               Indiana\n",
      "15                  Iowa\n",
      "16                Kansas\n",
      "17              Kentucky\n",
      "18             Louisiana\n",
      "19                 Maine\n",
      "20              Maryland\n",
      "21         Massachusetts\n",
      "22              Michigan\n",
      "23             Minnesota\n",
      "24           Mississippi\n",
      "25              Missouri\n",
      "26               Montana\n",
      "27              Nebraska\n",
      "28                Nevada\n",
      "29         New Hampshire\n",
      "30            New Jersey\n",
      "31            New Mexico\n",
      "32              New York\n",
      "33        North Carolina\n",
      "34          North Dakota\n",
      "35                  Ohio\n",
      "36              Oklahoma\n",
      "37                Oregon\n",
      "38          Pennsylvania\n",
      "39          Rhode Island\n",
      "40        South Carolina\n",
      "41          South Dakota\n",
      "42             Tennessee\n",
      "43                 Texas\n",
      "44                  Utah\n",
      "45               Vermont\n",
      "46              Virginia\n",
      "47            Washington\n",
      "48         West Virginia\n",
      "49             Wisconsin\n",
      "50               Wyoming\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import fnmatch\n",
    "\n",
    "# first pull the HTML from the page that links to all of the pages with the links.\n",
    "# in this case, this page gives the links list pages of county in the USA.\n",
    "\n",
    "website_url = requests.get('https://en.wikipedia.org/wiki/County_(United_States)').text\n",
    "\n",
    "soup = BeautifulSoup(website_url,'lxml')\n",
    "#print(soup.prettify())\n",
    "\n",
    "My_table = soup.find('table',{'class':'wikitable sortable'})\n",
    "links = My_table.findAll('a')\n",
    "\n",
    "State =[]\n",
    "for links in links:\n",
    "    State.append(links.get('title'))\n",
    "#display(State)  \n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['State'] = State\n",
    "df = df.drop_duplicates(keep=False, inplace=False)\n",
    "\n",
    "# replace incorrect state names\n",
    "replace_list = {'New York (state)':'New York','Washington (state)':'Washington', \n",
    "                'Georgia (U.S. state)':'Georgia','U.S. state':'District of Columbia'}\n",
    "df = df.replace(replace_list)\n",
    "\n",
    "# remove non USA state names\n",
    "remove_list = ('Administrative divisions of American Samoa','American Samoa', \n",
    "               'Guam','Municipalities of Puerto Rico','United States',\n",
    "               'None', 'Puerto Rico','Washington, D.C.')\n",
    "df = df[~df['State'].isin(remove_list)]\n",
    "\n",
    "# remove lists of non state names\n",
    "df = df[~df.State.str.contains('U.S.*' )]\n",
    "df = df[~df.State.str.contains('List of*' )]\n",
    "\n",
    "df = df.sort_values(by=['State']).drop_duplicates(keep=False, inplace=False)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#print(list(df))\n",
    "print(df)\n",
    "\n",
    "# Writting the new date to a file \n",
    "df.to_csv(\"C:/Users/Joe/wikipedia_state_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one way to obtain the list of states from wikipedia. \n",
    "File save to: C:/Users/Joe/wikipedia_state_list.csv\n",
    "Next up counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
